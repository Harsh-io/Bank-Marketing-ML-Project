





# Standard library imports
import sys
import os
import warnings
warnings.filterwarnings('ignore')

# Data manipulation
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Set plotting style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette('husl')

# Add src directory to path
sys.path.insert(0, os.path.abspath('..'))

# Import custom modules
from src.preprocessing import DataPreprocessor
from src.model_training import ModelTrainer
from src.model_evaluation import ModelEvaluator, generate_full_report

print("âœ“ All imports successful!")
print(f"\nPython version: {sys.version}")
print(f"Pandas version: {pd.__version__}")
print(f"NumPy version: {np.__version__}")





# Load the dataset
DATA_PATH = '../bank-additional-full.csv'

df = pd.read_csv(DATA_PATH, sep=';')

print(f"Dataset Shape: {df.shape}")
print(f"\nNumber of samples: {df.shape[0]:,}")
print(f"Number of features: {df.shape[1] - 1}")
print(f"Target variable: 'y' (subscribed to term deposit)")


# Display first few rows
df.head(10)


# Dataset info
print("\nDataset Information:")
print("="*50)
df.info()


# Statistical summary
print("\nStatistical Summary - Numerical Features:")
df.describe()


# Target variable distribution
print("\nTarget Variable Distribution:")
print("="*50)
target_counts = df['y'].value_counts()
target_pct = df['y'].value_counts(normalize=True) * 100

for val in target_counts.index:
    print(f"  {val}: {target_counts[val]:,} ({target_pct[val]:.2f}%)")

# Visualize target distribution
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Bar chart
colors = ['#e74c3c', '#2ecc71']
target_counts.plot(kind='bar', ax=axes[0], color=colors)
axes[0].set_title('Target Distribution (Count)')
axes[0].set_xlabel('Subscribed')
axes[0].set_ylabel('Count')
axes[0].tick_params(axis='x', rotation=0)

# Pie chart
axes[1].pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', 
            colors=colors, startangle=90)
axes[1].set_title('Target Distribution (Percentage)')

plt.tight_layout()
plt.show()

print("\nâš ï¸ Note: The dataset is imbalanced (~88.7% 'no' vs ~11.3% 'yes')")


# Analyze categorical features
categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 
                    'loan', 'contact', 'month', 'day_of_week', 'poutcome']

print("Categorical Features - Unique Values:")
print("="*50)
for col in categorical_cols:
    unique_vals = df[col].nunique()
    unknown_count = (df[col] == 'unknown').sum()
    unknown_pct = (unknown_count / len(df)) * 100
    print(f"  {col}: {unique_vals} unique values", end="")
    if unknown_count > 0:
        print(f" | 'unknown': {unknown_count:,} ({unknown_pct:.2f}%)")
    else:
        print()


# Visualize categorical features distribution
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

key_categorical = ['job', 'marital', 'education', 'contact', 'month', 'poutcome']

for idx, col in enumerate(key_categorical):
    df[col].value_counts().plot(kind='bar', ax=axes[idx])
    axes[idx].set_title(f'Distribution of {col}')
    axes[idx].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.suptitle('Categorical Features Distribution', y=1.02, fontsize=14)
plt.show()


# Visualize numerical features distribution
numerical_cols = ['age', 'duration', 'campaign', 'pdays', 'previous',
                  'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 
                  'euribor3m', 'nr.employed']

fig, axes = plt.subplots(2, 5, figsize=(20, 8))
axes = axes.flatten()

for idx, col in enumerate(numerical_cols):
    df[col].hist(bins=30, ax=axes[idx], edgecolor='black', alpha=0.7)
    axes[idx].set_title(f'{col}')
    axes[idx].set_xlabel('')

plt.tight_layout()
plt.suptitle('Numerical Features Distribution', y=1.02, fontsize=14)
plt.show()


# Correlation matrix for numerical features
plt.figure(figsize=(12, 10))

# Create a copy and encode target for correlation
df_corr = df.copy()
df_corr['target'] = (df_corr['y'] == 'yes').astype(int)

corr_matrix = df_corr[numerical_cols + ['target']].corr()

mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',
            center=0, square=True, linewidths=0.5)
plt.title('Correlation Matrix - Numerical Features', fontsize=14)
plt.tight_layout()
plt.show()





# Initialize the preprocessor
preprocessor = DataPreprocessor(random_state=42)

# Run the complete preprocessing pipeline
X_train, X_test, y_train, y_test = preprocessor.fit_transform(
    filepath=DATA_PATH,
    missing_strategy='keep',  # Keep 'unknown' as a category
    encoding_type='onehot',   # Use One-Hot Encoding
    test_size=0.2             # 80/20 train-test split
)


# Display preprocessed data info
print("\nPreprocessed Data Summary:")
print("="*50)
print(f"Training set shape: {X_train.shape}")
print(f"Testing set shape: {X_test.shape}")
print(f"\nNumber of features after encoding: {X_train.shape[1]}")

print(f"\nTraining target distribution:")
train_dist = y_train.value_counts(normalize=True) * 100
print(f"  Class 0 (No): {train_dist[0]:.2f}%")
print(f"  Class 1 (Yes): {train_dist[1]:.2f}%")

print(f"\nTesting target distribution:")
test_dist = y_test.value_counts(normalize=True) * 100
print(f"  Class 0 (No): {test_dist[0]:.2f}%")
print(f"  Class 1 (Yes): {test_dist[1]:.2f}%")


# Preview preprocessed features
print("\nSample of preprocessed training data:")
X_train.head()


# Save processed data
preprocessor.save_processed_data(
    X_train, X_test, y_train, y_test,
    output_dir='../data/processed'
)





# Initialize the model trainer
trainer = ModelTrainer(random_state=42)


# Train all models with 5-fold cross-validation
# Note: SVM is excluded by default for large datasets due to computational cost
# To include SVM, remove it from the exclude list

trained_models = trainer.train_all_models(
    X_train, y_train,
    cv=5,
    exclude=['SVM']  # Exclude SVM for faster training on large dataset
)


# Display training summary
training_summary = trainer.get_training_summary()
print("\nTraining Summary:")
training_summary


# Visualize cross-validation scores
fig, ax = plt.subplots(figsize=(10, 6))

models = list(trainer.cv_scores.keys())
cv_means = [trainer.cv_scores[m]['mean'] for m in models]
cv_stds = [trainer.cv_scores[m]['std'] for m in models]

bars = ax.bar(models, cv_means, yerr=cv_stds, capsize=5, 
              color=plt.cm.viridis(np.linspace(0.2, 0.8, len(models))))

ax.set_ylabel('Cross-Validation Accuracy')
ax.set_title('Model Comparison - Cross-Validation Scores')
ax.set_ylim([0.85, 0.95])
plt.xticks(rotation=45, ha='right')

# Add value labels on bars
for bar, mean in zip(bars, cv_means):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
            f'{mean:.4f}', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()





# Initialize the evaluator
evaluator = ModelEvaluator()

# Evaluate all trained models
results = evaluator.evaluate_all_models(
    trained_models, 
    X_train, y_train, 
    X_test, y_test
)


# Print detailed comparison table
evaluator.print_comparison_table()


# Plot confusion matrices for all models
evaluator.plot_confusion_matrices(
    figsize=(16, 12),
    save_path='../results/confusion_matrices.png'
)


# Plot ROC curves
evaluator.plot_roc_curves(
    y_test,
    figsize=(10, 8),
    save_path='../results/roc_curves.png'
)


# Plot metrics comparison
evaluator.plot_metrics_comparison(
    metrics=['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1-Score'],
    figsize=(14, 6),
    save_path='../results/metrics_comparison.png'
)


# Feature importance for the best tree-based model
# Using Random Forest as it typically provides stable feature importances

best_tree_model = trained_models.get('Random Forest')
if best_tree_model is not None:
    feature_importance = evaluator.plot_feature_importance(
        best_tree_model,
        'Random Forest',
        preprocessor.feature_names,
        top_n=20,
        figsize=(10, 8),
        save_path='../results/feature_importance.png'
    )
    
    print("\nTop 20 Most Important Features:")
    display(feature_importance)





# Get the final comparison table
comparison_df = evaluator.comparison_df

# Display with styling
print("\n" + "="*80)
print("FINAL MODEL COMPARISON TABLE")
print("="*80)

comparison_df


# Save comparison table
evaluator.save_comparison_table('../results/comparison_table.csv')

# Also create a formatted version
comparison_df_formatted = comparison_df.copy()
numeric_cols = comparison_df_formatted.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    comparison_df_formatted[col] = comparison_df_formatted[col].apply(lambda x: f"{x:.4f}")

comparison_df_formatted.to_csv('../results/comparison_table_formatted.csv', index=False)
print("âœ“ Formatted comparison table saved!")


# Detailed classification reports for all models
print("\n" + "="*80)
print("DETAILED CLASSIFICATION REPORTS")
print("="*80)

for model_name in trained_models.keys():
    print(f"\n{'='*40}")
    print(f"{model_name}")
    print('='*40)
    print(evaluator.get_classification_report(model_name, y_test))


# Create a summary visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# 1. Test Accuracy comparison
ax1 = axes[0, 0]
models = comparison_df['Model'].tolist()
test_acc = comparison_df['Test Accuracy'].tolist()
colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(models)))
bars1 = ax1.barh(models, test_acc, color=colors)
ax1.set_xlabel('Test Accuracy')
ax1.set_title('Test Accuracy by Model')
ax1.set_xlim([0.85, 0.95])
for bar, val in zip(bars1, test_acc):
    ax1.text(val + 0.002, bar.get_y() + bar.get_height()/2, 
             f'{val:.4f}', va='center', fontsize=9)

# 2. Test F1-Score comparison
ax2 = axes[0, 1]
test_f1 = comparison_df['Test F1-Score'].tolist()
bars2 = ax2.barh(models, test_f1, color=colors)
ax2.set_xlabel('Test F1-Score')
ax2.set_title('Test F1-Score by Model')
ax2.set_xlim([0.3, 0.6])
for bar, val in zip(bars2, test_f1):
    ax2.text(val + 0.005, bar.get_y() + bar.get_height()/2, 
             f'{val:.4f}', va='center', fontsize=9)

# 3. ROC-AUC comparison
ax3 = axes[1, 0]
test_auc = comparison_df['Test ROC-AUC'].tolist()
bars3 = ax3.barh(models, test_auc, color=colors)
ax3.set_xlabel('Test ROC-AUC')
ax3.set_title('Test ROC-AUC by Model')
ax3.set_xlim([0.7, 1.0])
for bar, val in zip(bars3, test_auc):
    ax3.text(val + 0.005, bar.get_y() + bar.get_height()/2, 
             f'{val:.4f}', va='center', fontsize=9)

# 4. Precision vs Recall scatter
ax4 = axes[1, 1]
test_precision = comparison_df['Test Precision'].tolist()
test_recall = comparison_df['Test Recall'].tolist()
scatter = ax4.scatter(test_precision, test_recall, c=range(len(models)), 
                      cmap='viridis', s=200, edgecolors='black')
for i, model in enumerate(models):
    ax4.annotate(model, (test_precision[i], test_recall[i]), 
                 textcoords="offset points", xytext=(5, 5), fontsize=8)
ax4.set_xlabel('Test Precision')
ax4.set_ylabel('Test Recall')
ax4.set_title('Precision vs Recall Trade-off')
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('../results/model_summary.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ“ Summary visualization saved to results/model_summary.png")





# Final summary
print("\n" + "="*80)
print("FINAL SUMMARY")
print("="*80)

best_model_name = evaluator.best_model_name
best_results = evaluator.results[best_model_name]

print(f"\nðŸ† BEST PERFORMING MODEL: {best_model_name}")
print("\nBest Model Metrics:")
print(f"  â€¢ Test Accuracy:  {best_results['test_accuracy']:.4f}")
print(f"  â€¢ Test Precision: {best_results['test_precision']:.4f}")
print(f"  â€¢ Test Recall:    {best_results['test_recall']:.4f}")
print(f"  â€¢ Test F1-Score:  {best_results['test_f1']:.4f}")
print(f"  â€¢ Test ROC-AUC:   {best_results['test_roc_auc']:.4f}")

print("\n" + "-"*80)
print("\nKey Findings:")
print("-"*80)
print(f"""\n1. Dataset Characteristics:
   - Total samples: {len(df):,}
   - Features after encoding: {X_train.shape[1]}
   - Class imbalance: ~88.7% 'No' vs ~11.3% 'Yes'

2. Model Performance:
   - All models achieved >85% accuracy
   - Gradient boosting methods (XGBoost, LightGBM, Gradient Boosting) 
     generally performed best
   - The class imbalance affects recall for the minority class

3. Recommendations:
   - Consider using {best_model_name} for production
   - For better recall, consider class weighting or SMOTE
   - Feature 'duration' is highly predictive but may cause data leakage""")

print("\n" + "="*80)
print("\nâœ“ Analysis Complete!")
print("\nResults saved to:")
print("  â€¢ results/comparison_table.csv")
print("  â€¢ results/confusion_matrices.png")
print("  â€¢ results/roc_curves.png")
print("  â€¢ results/metrics_comparison.png")
print("  â€¢ results/feature_importance.png")
print("  â€¢ results/model_summary.png")
